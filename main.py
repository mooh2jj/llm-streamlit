import os
# protobuf í˜¸í™˜ì„±ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ë‹¤ë¥¸ ëª¨ë“  importë³´ë‹¤ ë¨¼ì €!)
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
    print("<<<<< sqlite3 patched with pysqlite3 >>>>>")
except ImportError:
    # pysqlite3ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ sqlite3 ì‚¬ìš©
    print("<<<<< using default sqlite3 >>>>>")

print("<<<<< app.app.py IS BEING LOADED (sqlite3 patched with pysqlite3) >>>>>") # íŒ¨ì¹˜ ë‚´ìš© ëª…ì‹œ

import streamlit as st
import tempfile
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult
from typing import Dict, List, Any
import time

from streamlit_extras.buy_me_a_coffee import button

button(username="ehtjd33e", floating=True, width=221)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="ë²•ë¥ ê°€ ì±—ë´‡", page_icon=":books:", layout="wide")

st.title("ğŸ“š ë²•ë¥ ê°€ ì±—ë´‡")
st.caption("PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€ë°›ìœ¼ì„¸ìš”")

# OpenAI API í‚¤ ë¡œë“œ
openai_api_key = os.getenv("OPENAI_API_KEY")

# ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬
class StreamlitCallbackHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text + "â–‹")  # ì»¤ì„œ íš¨ê³¼
        time.sleep(0.01)  # ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        self.container.markdown(self.text)  # ìµœì¢… í…ìŠ¤íŠ¸ (ì»¤ì„œ ì œê±°)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
    
    # PDF íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf"],
        help="ë²•ë¥  ë¬¸ì„œë‚˜ ê´€ë ¨ ìë£Œë¥¼ PDF í˜•íƒœë¡œ ì—…ë¡œë“œí•˜ì„¸ìš”"
    )
    
    st.divider()
    
    st.header("âš™ï¸ ì„¤ì •")
    if not openai_api_key:
        st.error("âš ï¸ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        st.info("ğŸ“ .env íŒŒì¼ ì˜ˆì‹œ:\nOPENAI_API_KEY=your_api_key_here")
    else:
        st.success("âœ… OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    if uploaded_file:
        st.success(f"âœ… íŒŒì¼ ì—…ë¡œë“œë¨: {uploaded_file.name}")
        st.info(f"ğŸ“„ íŒŒì¼ í¬ê¸°: {uploaded_file.size / 1024:.1f} KB")

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
@st.cache_resource
def initialize_rag_system(file_path):
    """RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        # PDF ë¬¸ì„œ ë¡œë”©
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ ì§€ì›)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        
        # ê¸°ì¡´ ChromaDB ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ì‚­ì œ (ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°)
        import shutil
        if os.path.exists("./chroma_db"):
            shutil.rmtree("./chroma_db")

        # ChromaDB ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory="./chroma_db"
        )
        
        return vectorstore
    except Exception as e:
        st.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
def save_uploaded_file(uploaded_file):
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            return tmp_file.name
    except Exception as e:
        st.error(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# RAG ì§ˆì˜ì‘ë‹µ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
def get_rag_response_streaming(question, vectorstore, api_key, container):
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)."""
    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
        callback_handler = StreamlitCallbackHandler(container)
        
        # LLM ì„¤ì • (ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”)
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0,
            streaming=True,
            callbacks=[callback_handler]
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        prompt_template = """
        ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥¼ ê²½ìš° ì†”ì§íˆ ë§ì”€í•´ì£¼ì„¸ìš”.
        
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:
        """
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RetrievalQA ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        # ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
        response = qa_chain.invoke({"query": question})
        return callback_handler.text  # ìŠ¤íŠ¸ë¦¬ë°ëœ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
        
    except Exception as e:
        error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        container.error(error_msg)
        return error_msg

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None

if 'current_file' not in st.session_state:
    st.session_state.current_file = None

# íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
if uploaded_file:
    # ìƒˆ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if st.session_state.current_file != uploaded_file.name:
        st.session_state.current_file = uploaded_file.name
        st.session_state.vectorstore = None  # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        st.session_state.messages = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        
        # íŒŒì¼ ì €ì¥ ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        with st.spinner("ğŸ“„ PDF ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì„ë² ë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"):
            temp_file_path = save_uploaded_file(uploaded_file)
            if temp_file_path:
                st.session_state.vectorstore = initialize_rag_system(temp_file_path)
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                os.unlink(temp_file_path)
                
                if st.session_state.vectorstore:
                    st.success("âœ… ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    st.error("âŒ ë¬¸ì„œ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ë•Œ ì•ˆë‚´
if not uploaded_file:
    st.info("ğŸ“ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
    st.markdown("""
    ### ğŸ“‹ ì‚¬ìš© ë°©ë²•
    1. **íŒŒì¼ ì—…ë¡œë“œ**: ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ ì—…ë¡œë“œ
    2. **ë¬¸ì„œ ë¶„ì„**: ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ìë™ìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤
    3. **ì§ˆë¬¸í•˜ê¸°**: ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”
    
    ### ğŸ“„ ì§€ì› íŒŒì¼ í˜•ì‹
    - PDF íŒŒì¼ë§Œ ì§€ì›ë©ë‹ˆë‹¤
    - í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤
    """)
else:
    # ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_question := st.chat_input(placeholder="ì—…ë¡œë“œëœ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”... ğŸ’¬"):
        # API í‚¤ í™•ì¸
        if not openai_api_key:
            st.error("OpenAI API í‚¤ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”! .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        elif st.session_state.vectorstore is None:
            st.error("ë¬¸ì„œ ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        else:
            # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
            with st.chat_message("user"):
                st.markdown(user_question)
            st.session_state.messages.append({"role": "user", "content": user_question})

            # AI ë‹µë³€ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ
            with st.chat_message("assistant"):
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë¹ˆ ì»¨í…Œì´ë„ˆ ìƒì„±
                message_container = st.empty()
                
                # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ìƒì„±
                ai_response = get_rag_response_streaming(
                    user_question, 
                    st.session_state.vectorstore, 
                    openai_api_key,
                    message_container
                )
                
            st.session_state.messages.append({"role": "assistant", "content": ai_response})

# í•˜ë‹¨ì— ì‚¬ìš© íŒ ì¶”ê°€
if uploaded_file:
    st.divider()
    with st.expander("ğŸ’¡ ì‚¬ìš© íŒ"):
        st.markdown(f"""
        **í˜„ì¬ ë¶„ì„ ì¤‘ì¸ íŒŒì¼: {uploaded_file.name}**
        
        **ì´ ì±—ë´‡ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?**
        - ğŸ“„ ì—…ë¡œë“œëœ PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
        - ğŸ” ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
        - âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ì´ ìƒì„±ë©ë‹ˆë‹¤
        
        **ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•œ íŒ:**
        - êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”
        - ë¬¸ì„œì˜ íŠ¹ì • ë¶€ë¶„ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”
        - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ "ëª¨ë¥¸ë‹¤"ê³  ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        """)

    # ë””ë²„ê¹…ìš© (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
    if st.checkbox("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ"):
        st.json({"ë©”ì‹œì§€ ê°œìˆ˜": len(st.session_state.messages)})
        with st.expander("ì „ì²´ ëŒ€í™” ë‚´ì—­"):
            st.json(st.session_state.messages)