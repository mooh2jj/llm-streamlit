import os
# protobuf í˜¸í™˜ì„±ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ë‹¤ë¥¸ ëª¨ë“  importë³´ë‹¤ ë¨¼ì €!)
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
    print("<<<<< sqlite3 patched with pysqlite3 >>>>>")
except ImportError:
    # pysqlite3ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ sqlite3 ì‚¬ìš©
    print("<<<<< using default sqlite3 >>>>>")

print("<<<<< app.app.py IS BEING LOADED (sqlite3 patched with pysqlite3) >>>>>") # íŒ¨ì¹˜ ë‚´ìš© ëª…ì‹œ

import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult
from typing import Dict, List, Any
import time

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="ë²•ë¥ ê°€ ì±—ë´‡", page_icon=":books:", layout="wide")

st.title("ğŸ“š ë²•ë¥ ê°€ ì±—ë´‡")
st.caption("ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤")

# OpenAI API í‚¤ ë¡œë“œ
openai_api_key = os.getenv("OPENAI_API_KEY")

# ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬
class StreamlitCallbackHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text + "â–‹")  # ì»¤ì„œ íš¨ê³¼
        time.sleep(0.01)  # ìì—°ìŠ¤ëŸ¬ìš´ íƒ€ì´í•‘ íš¨ê³¼

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        self.container.markdown(self.text)  # ìµœì¢… í…ìŠ¤íŠ¸ (ì»¤ì„œ ì œê±°)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ì„¤ì •")
    if not openai_api_key:
        st.error("âš ï¸ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        st.info("ğŸ“ .env íŒŒì¼ ì˜ˆì‹œ:\nOPENAI_API_KEY=your_api_key_here")
    else:
        st.success("âœ… OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
@st.cache_resource
def initialize_rag_system():
    """RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        # PDF ë¬¸ì„œ ë¡œë”©
        loader = PyPDFLoader("tax.pdf")
        documents = loader.load()
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (í•œêµ­ì–´ ì§€ì›)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        
        # ê¸°ì¡´ ChromaDB ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ì‚­ì œ (ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°)
        import shutil
        if os.path.exists("./chroma_db"):
            shutil.rmtree("./chroma_db")

        # ChromaDB ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory="./chroma_db"
        )
        
        return vectorstore
    except Exception as e:
        st.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# RAG ì§ˆì˜ì‘ë‹µ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
def get_rag_response_streaming(question, vectorstore, api_key, container):
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)."""
    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
        callback_handler = StreamlitCallbackHandler(container)
        
        # LLM ì„¤ì • (ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”)
        llm = ChatOpenAI(
            openai_api_key=api_key,
            model_name="gpt-4o-mini",
            temperature=0,
            streaming=True,
            callbacks=[callback_handler]
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        prompt_template = """
        ë‹¹ì‹ ì€ ì„¸ë¬´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥¼ ê²½ìš° ì†”ì§íˆ ë§ì”€í•´ì£¼ì„¸ìš”.
        
        ë¬¸ì„œ ë‚´ìš©:
        {context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:
        """
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # RetrievalQA ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        # ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
        response = qa_chain.invoke({"query": question})
        return callback_handler.text  # ìŠ¤íŠ¸ë¦¬ë°ëœ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
        
    except Exception as e:
        error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        container.error(error_msg)
        return error_msg

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'vectorstore' not in st.session_state:
    with st.spinner("ğŸ“„ PDF ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"):
        st.session_state.vectorstore = initialize_rag_system()

# ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_question := st.chat_input(placeholder="ì„¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”... ğŸ’¬"):
    # API í‚¤ í™•ì¸
    if not openai_api_key:
        st.error("OpenAI API í‚¤ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”! .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    elif st.session_state.vectorstore is None:
        st.error("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
        with st.chat_message("user"):
            st.markdown(user_question)
        st.session_state.messages.append({"role": "user", "content": user_question})

        # AI ë‹µë³€ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ
        with st.chat_message("assistant"):
            # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë¹ˆ ì»¨í…Œì´ë„ˆ ìƒì„±
            message_container = st.empty()
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ ìƒì„±
            ai_response = get_rag_response_streaming(
                user_question, 
                st.session_state.vectorstore, 
                openai_api_key,
                message_container
            )
            
        st.session_state.messages.append({"role": "assistant", "content": ai_response})

# í•˜ë‹¨ì— ì‚¬ìš© íŒ ì¶”ê°€
st.divider()
with st.expander("ğŸ’¡ ì‚¬ìš© íŒ"):
    st.markdown("""
    **ì´ ì±—ë´‡ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?**
    - ğŸ“„ PDF ë¬¸ì„œ(tax.pdf)ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤
    - ğŸ” ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
    - âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ì´ ìƒì„±ë©ë‹ˆë‹¤
    
    **ë” ë‚˜ì€ ë‹µë³€ì„ ìœ„í•œ íŒ:**
    - êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”
    - ì„¸ë¬´ ê´€ë ¨ ì „ë¬¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ë„ ì¢‹ìŠµë‹ˆë‹¤
    - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ "ëª¨ë¥¸ë‹¤"ê³  ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """)

# ë””ë²„ê¹…ìš© (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
if st.checkbox("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ"):
    st.json({"ë©”ì‹œì§€ ê°œìˆ˜": len(st.session_state.messages)})
    with st.expander("ì „ì²´ ëŒ€í™” ë‚´ì—­"):
        st.json(st.session_state.messages)